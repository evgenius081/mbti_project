{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (MBTI) Myers-Briggs Personality Type Prediction\n",
    "\n",
    "* Extroversion vs. Introversion\n",
    "    * I - 0\n",
    "    * E - 1 \n",
    "    \n",
    "* Sensing vs. Intuition \n",
    "    * N - 0 \n",
    "    * S - 1\n",
    "    \n",
    "* Thinking vs. Feeling\n",
    "    * F - 0\n",
    "    * T - 1\n",
    "    \n",
    "* Judging vs. Perceiving\n",
    "    * P - 0\n",
    "    * J - 1 \n",
    "    \n",
    "\n",
    "## SENTIMENT ANALYSIS & PART OF SPEECH TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:21:25.475647Z",
     "end_time": "2023-04-18T17:21:26.614834Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gulev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gulev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# importing dependencies here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# feature engineering\n",
    "import re\n",
    "\n",
    "# pos tagging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# sentiment scoring\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# scaling to handle negative values in sentiment scores (for Naive Bayes)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# performance check\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:21:26.614834Z",
     "end_time": "2023-04-18T17:21:29.577684Z"
    }
   },
   "outputs": [],
   "source": [
    "# reading the clean_dataset_1\n",
    "personality_data = pd.read_csv(os.path.join(\"..\", \"data\", \"clean_data_1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:21:29.584320Z",
     "end_time": "2023-04-18T17:21:29.604283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   type  is_Extrovert  is_Sensing  is_Thinking  is_Judging   \n0  INTJ             0           0            1           1  \\\n1  INTP             0           0            1           0   \n2  INFJ             0           0            0           1   \n3  INTP             0           0            1           0   \n4  INTJ             0           0            1           1   \n\n                                               posts   \n0  'Don’t peg your rate low, peg it high. An appe...  \\\n1  '...you get Kid Rock.|||Now it is. Plural: 2sh...   \n2  'To me it seems that Stoicism has been reduced...   \n3  'unBELIEVABLY based...same with physics|||yeah...   \n4  'Tbh, Ne doms are very fun chaotic people that...   \n\n                                         clean_posts  \n0   peg rate low  peg high  appeal expertise want...  \n1      get kid rock   plural   shy happened skim ...  \n2   seems stoicism reduced colloquial usage  beco...  \n3   unbelievably based   physic yeah got material...  \n4   tbh   doms fun chaotic people talk hour   mig...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>is_Extrovert</th>\n      <th>is_Sensing</th>\n      <th>is_Thinking</th>\n      <th>is_Judging</th>\n      <th>posts</th>\n      <th>clean_posts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>INTJ</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>'Don’t peg your rate low, peg it high. An appe...</td>\n      <td>peg rate low  peg high  appeal expertise want...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>INTP</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>'...you get Kid Rock.|||Now it is. Plural: 2sh...</td>\n      <td>get kid rock   plural   shy happened skim ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>INFJ</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>'To me it seems that Stoicism has been reduced...</td>\n      <td>seems stoicism reduced colloquial usage  beco...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>INTP</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>'unBELIEVABLY based...same with physics|||yeah...</td>\n      <td>unbelievably based   physic yeah got material...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>INTJ</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>'Tbh, Ne doms are very fun chaotic people that...</td>\n      <td>tbh   doms fun chaotic people talk hour   mig...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at the top 5 rows of the dataset\n",
    "personality_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:21:29.604283Z",
     "end_time": "2023-04-18T17:21:29.738958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(3933, 7)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the number of rows and columns\n",
    "personality_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:21:29.683974Z",
     "end_time": "2023-04-18T17:21:29.738958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "type            0\nis_Extrovert    0\nis_Sensing      0\nis_Thinking     0\nis_Judging      0\nposts           0\nclean_posts     0\ndtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values\n",
    "personality_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values present in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiments Analysis Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CAUTION - Sentiment scoring will take LONG !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:21:29.695297Z",
     "end_time": "2023-04-18T18:09:03.099677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Scoring Time: 2853.37 seconds\n"
     ]
    }
   ],
   "source": [
    "# sentiment scoring for each user\n",
    "t = time.time()\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "nlp_sentiment_score = []\n",
    "\n",
    "for post in personality_data[\"clean_posts\"]:\n",
    "    score = analyzer.polarity_scores(post)\n",
    "    nlp_sentiment_score.append(score)\n",
    "\n",
    "print(f\"Sentiment Scoring Time: {time.time() - t:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T18:09:03.101677Z",
     "end_time": "2023-04-18T18:09:03.173968Z"
    }
   },
   "outputs": [],
   "source": [
    "# segregating the indiviual sentiment scores - compound, positive, negative and neutral\n",
    "personality_data[\"compound_sentiment\"] = [\n",
    "    score[\"compound\"] for score in nlp_sentiment_score\n",
    "]\n",
    "personality_data[\"pos_sentiment\"] = [score[\"pos\"] for score in nlp_sentiment_score]\n",
    "personality_data[\"neg_sentiment\"] = [score[\"neg\"] for score in nlp_sentiment_score]\n",
    "personality_data[\"neu_sentiment\"] = [score[\"neu\"] for score in nlp_sentiment_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T18:09:03.133766Z",
     "end_time": "2023-04-18T18:09:03.214741Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sentiment scores have negative values that Naive Bayes can't handle. So scaling it.\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "personality_data[\"compound_sentiment\"] = min_max_scaler.fit_transform(\n",
    "    np.array(personality_data[\"compound_sentiment\"]).reshape(-1, 1)\n",
    ")\n",
    "personality_data[\"pos_sentiment\"] = min_max_scaler.fit_transform(\n",
    "    np.array(personality_data[\"pos_sentiment\"]).reshape(-1, 1)\n",
    ")\n",
    "personality_data[\"neg_sentiment\"] = min_max_scaler.fit_transform(\n",
    "    np.array(personality_data[\"neg_sentiment\"]).reshape(-1, 1)\n",
    ")\n",
    "personality_data[\"neu_sentiment\"] = min_max_scaler.fit_transform(\n",
    "    np.array(personality_data[\"neu_sentiment\"]).reshape(-1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T18:09:03.216767Z",
     "end_time": "2023-04-18T18:09:03.273356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "type                  0\nis_Extrovert          0\nis_Sensing            0\nis_Thinking           0\nis_Judging            0\nposts                 0\nclean_posts           0\ncompound_sentiment    0\npos_sentiment         0\nneg_sentiment         0\nneu_sentiment         0\ndtype: int64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking to see if sentiment scores introduced any null value\n",
    "personality_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T18:09:03.276386Z",
     "end_time": "2023-04-18T18:09:04.022288Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating tag_posts column that will have each post as a separate list in a row. tag_posts will be a list of 50 lists.\n",
    "\n",
    "# replacing urls with domain name\n",
    "personality_data[\"tag_posts\"] = personality_data[\"posts\"].str.replace(\n",
    "    re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\"),\n",
    "    lambda match: match.group(2),\n",
    "    regex=True\n",
    ")\n",
    "\n",
    "# replacing ||| with space\n",
    "personality_data[\"tag_posts\"] = [\n",
    "    post for post in personality_data[\"tag_posts\"].str.split(\"\\|\\|\\|\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CAUTION - The next step i.e. Parts of speech tagging for each word will take SUPER LONG !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T18:09:04.028288Z",
     "end_time": "2023-04-18T18:25:43.217820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging Time: 999.1841609477997 seconds\n"
     ]
    }
   ],
   "source": [
    "# parts of speech tagging for each word\n",
    "t = time.time()\n",
    "\n",
    "personality_data[\"tagged_words\"] = personality_data[\"tag_posts\"].apply(\n",
    "    lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\n",
    ")\n",
    "\n",
    "print(f\"POS Tagging Time: {time.time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T18:25:43.223807Z",
     "end_time": "2023-04-18T18:25:43.261292Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating list of unique POS tags\n",
    "tag_set = set()\n",
    "\n",
    "for i, data in personality_data[\"tagged_words\"].items():\n",
    "    for tup in data[0]:\n",
    "        tag_set.add(tup[1])\n",
    "\n",
    "tag_list = list(tag_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T18:25:43.262297Z",
     "end_time": "2023-04-18T18:27:50.789553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Stats Time: 127.52453589439392 seconds\n"
     ]
    }
   ],
   "source": [
    "# calculating mean and standard deviation of pos tags for each user\n",
    "t = time.time()\n",
    "\n",
    "\n",
    "def pos_cat(x, tag):\n",
    "    return [len([y for y in line if y[1] == tag]) for line in x]\n",
    "\n",
    "\n",
    "for col in tag_list:\n",
    "    personality_data[\"POS_\" + col + \"_mean\"] = personality_data[\"tagged_words\"].apply(\n",
    "        lambda x: np.mean(pos_cat(x, col))\n",
    "    )\n",
    "    personality_data[\"POS_\" + col + \"_std\"] = personality_data[\"tagged_words\"].apply(\n",
    "        lambda x: np.std(pos_cat(x, col))\n",
    "    )\n",
    "\n",
    "print(f\"POS Stats Time: {time.time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T18:27:50.804404Z",
     "end_time": "2023-04-18T18:27:50.819517Z"
    }
   },
   "outputs": [],
   "source": [
    "# grouping pos tags based on stanford list\n",
    "tags_dict = {\n",
    "    \"ADJ\": [\"JJ\", \"JJR\", \"JJS\"],\n",
    "    \"ADP\": [\"EX\", \"TO\"],\n",
    "    \"ADV\": [\"RB\", \"RBR\", \"RBS\", \"WRB\"],\n",
    "    \"CONJ\": [\"CC\", \"IN\"],\n",
    "    \"DET\": [\"DT\", \"PDT\", \"WDT\"],\n",
    "    \"NOUN\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
    "    \"NUM\": [\"CD\"],\n",
    "    \"PRT\": [\"RP\"],\n",
    "    \"PRON\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
    "    \"VERB\": [\"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
    "    \".\": [\"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\"],\n",
    "    \"X\": [\"FW\", \"LS\", \"UH\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T18:27:50.811520Z",
     "end_time": "2023-04-18T18:28:24.741215Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gulev\\AppData\\Local\\Temp\\ipykernel_25308\\258308816.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "C:\\Users\\gulev\\AppData\\Local\\Temp\\ipykernel_25308\\258308816.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "C:\\Users\\gulev\\AppData\\Local\\Temp\\ipykernel_25308\\258308816.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "C:\\Users\\gulev\\AppData\\Local\\Temp\\ipykernel_25308\\258308816.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "C:\\Users\\gulev\\AppData\\Local\\Temp\\ipykernel_25308\\258308816.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "C:\\Users\\gulev\\AppData\\Local\\Temp\\ipykernel_25308\\258308816.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "C:\\Users\\gulev\\AppData\\Local\\Temp\\ipykernel_25308\\258308816.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford POS Stats Time: 33.8388512134552 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gulev\\AppData\\Local\\Temp\\ipykernel_25308\\258308816.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n"
     ]
    }
   ],
   "source": [
    "# Stanford POS tag stats\n",
    "t = time.time()\n",
    "\n",
    "\n",
    "def stanford_tag(x, tag):\n",
    "    tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\n",
    "    return tags_list\n",
    "\n",
    "\n",
    "for col in tags_dict.keys():\n",
    "    personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
    "        lambda x: np.median(stanford_tag(x, col))\n",
    "    )\n",
    "\n",
    "print(f\"Stanford POS Stats Time: {time.time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T18:28:24.744339Z",
     "end_time": "2023-04-18T18:28:24.757401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   type  is_Extrovert  is_Sensing  is_Thinking  is_Judging   \n0  INTJ             0           0            1           1  \\\n1  INTP             0           0            1           0   \n\n                                               posts   \n0  'Don’t peg your rate low, peg it high. An appe...  \\\n1  '...you get Kid Rock.|||Now it is. Plural: 2sh...   \n\n                                         clean_posts  compound_sentiment   \n0   peg rate low  peg high  appeal expertise want...             0.99990  \\\n1      get kid rock   plural   shy happened skim ...             0.99995   \n\n   pos_sentiment  neg_sentiment  ...  ADV_avg CONJ_avg DET_avg  NOUN_avg   \n0       0.332011       0.098166  ...      2.0      3.0     2.0       6.0  \\\n1       0.312169       0.088457  ...      4.0      7.0     5.0      13.0   \n\n   NUM_avg  PRT_avg  PRON_avg  VERB_avg  ._avg  X_avg  \n0      0.0      0.0       3.0       6.0    4.0    0.0  \n1      0.0      0.0       4.0      13.0    8.0    0.0  \n\n[2 rows x 113 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>is_Extrovert</th>\n      <th>is_Sensing</th>\n      <th>is_Thinking</th>\n      <th>is_Judging</th>\n      <th>posts</th>\n      <th>clean_posts</th>\n      <th>compound_sentiment</th>\n      <th>pos_sentiment</th>\n      <th>neg_sentiment</th>\n      <th>...</th>\n      <th>ADV_avg</th>\n      <th>CONJ_avg</th>\n      <th>DET_avg</th>\n      <th>NOUN_avg</th>\n      <th>NUM_avg</th>\n      <th>PRT_avg</th>\n      <th>PRON_avg</th>\n      <th>VERB_avg</th>\n      <th>._avg</th>\n      <th>X_avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>INTJ</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>'Don’t peg your rate low, peg it high. An appe...</td>\n      <td>peg rate low  peg high  appeal expertise want...</td>\n      <td>0.99990</td>\n      <td>0.332011</td>\n      <td>0.098166</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>INTP</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>'...you get Kid Rock.|||Now it is. Plural: 2sh...</td>\n      <td>get kid rock   plural   shy happened skim ...</td>\n      <td>0.99995</td>\n      <td>0.312169</td>\n      <td>0.088457</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>13.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>13.0</td>\n      <td>8.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 113 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a quick look at the data\n",
    "personality_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T18:28:24.757401Z",
     "end_time": "2023-04-18T18:28:51.184699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sentiment scoring & POS Tagging took long. So saving the scored & tagged file to save time in the next step.\n",
    "personality_data.to_csv(os.path.join(\"..\", \"data\", \"clean_data_2.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:21:20.684061Z",
     "end_time": "2023-04-18T18:28:51.185718Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
