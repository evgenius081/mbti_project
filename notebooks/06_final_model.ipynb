{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (MBTI) Myers-Briggs Personality Type Prediction\n",
    "\n",
    "* Extroversion vs. Introversion\n",
    "    * I - 0\n",
    "    * E - 1 \n",
    "    \n",
    "* Sensing vs. Intuition \n",
    "    * N - 0 \n",
    "    * S - 1\n",
    "    \n",
    "* Thinking vs. Feeling\n",
    "    * F - 0\n",
    "    * T - 1\n",
    "    \n",
    "* Judging vs. Perceiving\n",
    "    * P - 0\n",
    "    * J - 1 \n",
    "    \n",
    "## FINAL MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T09:10:03.718931Z",
     "end_time": "2023-04-22T09:10:06.368846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gulev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "# importing dependencies here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# feature engineering\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# sentiment scoring\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# pos tagging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "print(sklearn.__version__)\n",
    "\n",
    "# accuracy scores\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "\n",
    "# performance check\n",
    "import time\n",
    "\n",
    "# sparse to dense\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "\n",
    "# importing model\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting for the Holdout Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T09:10:06.370846Z",
     "end_time": "2023-04-22T09:10:06.463046Z"
    }
   },
   "outputs": [],
   "source": [
    "# reading the test dataset\n",
    "path_to_csv = os.path.join(\"..\", \"data\", \"df_holdout.csv\")\n",
    "df = pd.read_csv(path_to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T09:10:06.409665Z",
     "end_time": "2023-04-22T09:10:06.500452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   type                                              posts\n0  INTP  'Well, there are two of my gang, so I can tell...\n1  ENTJ  'How long have you been with your human? ðŸ˜º|||E...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>posts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>INTP</td>\n      <td>'Well, there are two of my gang, so I can tell...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENTJ</td>\n      <td>'How long have you been with your human? ðŸ˜º|||E...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking top records\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T09:10:06.429630Z",
     "end_time": "2023-04-22T09:10:06.506667Z"
    }
   },
   "outputs": [],
   "source": [
    "def categorize_types(personality_data):\n",
    "\n",
    "    personality_data[\"is_Extrovert\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[0] == \"E\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Sensing\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[1] == \"S\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Thinking\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[2] == \"T\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Judging\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[3] == \"J\" else 0\n",
    "    )\n",
    "\n",
    "    # rearranging the dataframe columns\n",
    "    personality_data = personality_data[\n",
    "        [\"type\", \"is_Extrovert\", \"is_Sensing\", \"is_Thinking\", \"is_Judging\", \"posts\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "#######################################################################################################3\n",
    "\n",
    "\n",
    "def clean_posts(personality_data):\n",
    "\n",
    "    # converting posts into lower case\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"posts\"].str.lower()\n",
    "\n",
    "    # replacing ||| with space\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"\\|\\|\\|\"), \" \", regex=True\n",
    "    )\n",
    "\n",
    "    # replacing urls with domain name\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\"),\n",
    "        \"\",\n",
    "        regex=True\n",
    "    )\n",
    "\n",
    "    # dropping emails\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"\\S+@\\S+\"), \"\", regex=True\n",
    "    )\n",
    "\n",
    "    # dropping punctuations\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"[^a-z\\s]\"), \" \", regex=True\n",
    "    )\n",
    "\n",
    "    # dropping MBTIs mentioned in the posts. There are quite a few mention of these types in these posts.\n",
    "    mbti = personality_data[\"type\"].unique()\n",
    "    for type_word in mbti:\n",
    "        personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "            type_word.lower(), \"\", regex=True\n",
    "        )\n",
    "        \n",
    "    # removing words that are 1 to 2 characters long    \n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "    re.compile(r\"\\b\\w{1,2}\\b\"), \"\", regex=True\n",
    "    )\n",
    "\n",
    "    # lemmitizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].apply(\n",
    "        lambda x: \" \".join(\n",
    "            [\n",
    "                lemmatizer.lemmatize(word)\n",
    "                for word in x.split(\" \")\n",
    "                if word not in stopwords.words(\"english\")\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # tag_posts will be a list of 50 lists. need it for word stats (per post for each user)\n",
    "    # replacing urls with domain name\n",
    "    personality_data[\"tag_posts\"] = personality_data[\"posts\"].str.replace(\n",
    "        re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\"),\n",
    "        lambda match: match.group(2),\n",
    "        regex=True\n",
    "    )\n",
    "    # replacing ||| with space\n",
    "    personality_data[\"tag_posts\"] = [\n",
    "        post for post in personality_data[\"tag_posts\"].str.split(\"\\|\\|\\|\")\n",
    "    ]\n",
    "\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "\n",
    "def sentiment_score(personality_data):\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    nlp_sentiment_score = []\n",
    "\n",
    "    for post in personality_data[\"clean_posts\"]:\n",
    "        score = analyzer.polarity_scores(post)[\"compound\"]\n",
    "        nlp_sentiment_score.append(score)\n",
    "\n",
    "    personality_data[\"compound_sentiment\"] = nlp_sentiment_score\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "\n",
    "def pos_tagging(personality_data):\n",
    "\n",
    "    personality_data[\"tagged_words\"] = personality_data[\"tag_posts\"].apply(\n",
    "        lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\n",
    "    )\n",
    "\n",
    "    # grouping pos tags based on stanford list\n",
    "    tags_dict = {\n",
    "        \"ADJ\": [\"JJ\", \"JJR\", \"JJS\"],\n",
    "        \"ADP\": [\"EX\", \"TO\"],\n",
    "        \"ADV\": [\"RB\", \"RBR\", \"RBS\", \"WRB\"],\n",
    "        \"CONJ\": [\"CC\", \"IN\"],\n",
    "        \"DET\": [\"DT\", \"PDT\", \"WDT\"],\n",
    "        \"NOUN\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
    "        \"NUM\": [\"CD\"],\n",
    "        \"PRT\": [\"RP\"],\n",
    "        \"PRON\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
    "        \"VERB\": [\"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
    "        \".\": [\"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\"],\n",
    "        \"X\": [\"FW\", \"LS\", \"UH\"],\n",
    "    }\n",
    "\n",
    "    def stanford_tag(x, tag):\n",
    "        tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\n",
    "        return tags_list\n",
    "\n",
    "    for col in tags_dict.keys():\n",
    "        personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
    "            lambda x: np.mean(stanford_tag(x, col))\n",
    "        )\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "\n",
    "def get_counts(personality_data):\n",
    "    def unique_words(s):\n",
    "        unique = set(s.split(\" \"))\n",
    "        return len(unique)/50\n",
    "\n",
    "    def emojis(post):\n",
    "        # does not include emojis made purely from symbols, only :word:\n",
    "        emoji_count = 0\n",
    "        words = post.split()\n",
    "        for e in words:\n",
    "            if \"http\" not in e:\n",
    "                if e.count(\":\") == 2:\n",
    "                    emoji_count += 1\n",
    "        return emoji_count/50\n",
    "\n",
    "    def colons(post):\n",
    "        # Includes colons used in emojis\n",
    "        colon_count = 0\n",
    "        words = post.split()\n",
    "        for e in words:\n",
    "            if \"http\" not in e:\n",
    "                colon_count += e.count(\":\")\n",
    "        return colon_count/50\n",
    "\n",
    "    personality_data[\"qm\"] = personality_data[\"posts\"].apply(lambda s: s.count(\"?\")/50)\n",
    "    personality_data[\"em\"] = personality_data[\"posts\"].apply(lambda s: s.count(\"!\")/50)\n",
    "    personality_data[\"colons\"] = personality_data[\"posts\"].apply(colons)\n",
    "    personality_data[\"emojis\"] = personality_data[\"posts\"].apply(emojis)\n",
    "\n",
    "    personality_data[\"word_count\"] = personality_data[\"posts\"].apply(\n",
    "        lambda s: (s.count(\" \") + 1)/50\n",
    "    )\n",
    "    personality_data[\"unique_words\"] = personality_data[\"posts\"].apply(unique_words)\n",
    "\n",
    "    personality_data[\"upper\"] = personality_data[\"posts\"].apply(\n",
    "        lambda x: len([x for x in x.split() if x.isupper()])/50\n",
    "    )\n",
    "    personality_data[\"link_count\"] = personality_data[\"posts\"].apply(\n",
    "        lambda s: s.count(\"http\")/50\n",
    "    )\n",
    "    ellipses_count = [\n",
    "        len(re.findall(r\"\\.\\.\\.\\ \", posts))/50 for posts in personality_data[\"posts\"]\n",
    "    ]\n",
    "    personality_data[\"ellipses\"] = ellipses_count\n",
    "    personality_data[\"img_count\"] = [\n",
    "        len(re.findall(r\"(\\.jpg)|(\\.jpeg)|(\\.gif)|(\\.png)\", post))/50\n",
    "        for post in personality_data[\"posts\"]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T09:10:06.455971Z",
     "end_time": "2023-04-22T09:10:06.507674Z"
    }
   },
   "outputs": [],
   "source": [
    "def prep_data(personality_data):\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    categorize_types(personality_data)\n",
    "\n",
    "    clean_posts(personality_data)\n",
    "\n",
    "    sentiment_score(personality_data)\n",
    "\n",
    "    pos_tagging(personality_data)\n",
    "\n",
    "    get_counts(personality_data)\n",
    "\n",
    "    features = personality_data[\n",
    "        [\n",
    "            \"clean_posts\",\n",
    "            \"compound_sentiment\",\n",
    "            \"ADJ_avg\",\n",
    "            \"ADP_avg\",\n",
    "            \"ADV_avg\",\n",
    "            \"CONJ_avg\",\n",
    "            \"DET_avg\",\n",
    "            \"NOUN_avg\",\n",
    "            \"NUM_avg\",\n",
    "            \"PRT_avg\",\n",
    "            \"PRON_avg\",\n",
    "            \"VERB_avg\",\n",
    "            \"qm\",\n",
    "            \"em\",\n",
    "            \"colons\",\n",
    "            \"emojis\",\n",
    "            \"word_count\",\n",
    "            \"unique_words\",\n",
    "            \"upper\",\n",
    "            \"link_count\",\n",
    "            \"ellipses\",\n",
    "            \"img_count\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    X = features\n",
    "    y = personality_data.iloc[:, 2:6]\n",
    "\n",
    "    print(f\"Total Preprocessing Time: {time.time()-t} seconds\\n\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T09:10:06.477362Z",
     "end_time": "2023-04-22T09:10:06.508675Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine_classes(y_pred1, y_pred2, y_pred3, y_pred4):\n",
    "    \n",
    "    combined = []\n",
    "    for i in range(len(y_pred1)):\n",
    "        combined.append(\n",
    "            str(y_pred1[i]) + str(y_pred2[i]) + str(y_pred3[i]) + str(y_pred4[i])\n",
    "        )\n",
    "    \n",
    "    result = trace_back(combined)\n",
    "    return result\n",
    "    \n",
    "\n",
    "def trace_back(combined):\n",
    "        \n",
    "    type_list = [\n",
    "    {\"0\": \"I\", \"1\": \"E\"},\n",
    "    {\"0\": \"N\", \"1\": \"S\"},\n",
    "    {\"0\": \"F\", \"1\": \"T\"},\n",
    "    {\"0\": \"P\", \"1\": \"J\"},\n",
    "    ]\n",
    "\n",
    "    result = []\n",
    "    for num in combined:\n",
    "        s = \"\"\n",
    "        for i in range(len(num)):\n",
    "            s += type_list[i][num[i]]\n",
    "        result.append(s)\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T09:10:06.490050Z",
     "end_time": "2023-04-22T09:10:06.567460Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(path_to_csv):\n",
    "\n",
    "    df = pd.read_csv(path_to_csv)\n",
    "\n",
    "    X, y = prep_data(df)\n",
    "\n",
    "    # loading the 4 models\n",
    "    EorI_model = load(os.path.join(\"..\", \"models\", \"clf_is_Extrovert.joblib\"))\n",
    "    SorN_model = load(os.path.join(\"..\", \"models\", \"clf_is_Sensing.joblib\"))\n",
    "    TorF_model = load(os.path.join(\"..\", \"models\", \"clf_is_Thinking.joblib\"))\n",
    "    JorP_model = load(os.path.join(\"..\", \"models\", \"clf_is_Judging.joblib\"))\n",
    "\n",
    "    # predicting\n",
    "    EorI_pred = EorI_model.predict(X)\n",
    "    print(\n",
    "        \"Extrovert vs Introvert Accuracy: \",\n",
    "        accuracy_score(y[\"is_Extrovert\"], EorI_pred),\n",
    "    )\n",
    "    print(\"y_true\", y[\"is_Extrovert\"].values)\n",
    "    print(\"preds\", EorI_pred)\n",
    "\n",
    "    SorN_pred = SorN_model.predict(X)\n",
    "    print(\n",
    "        \"\\nSensing vs Intuition Accuracy: \", accuracy_score(y[\"is_Sensing\"], SorN_pred)\n",
    "    )\n",
    "    print(\"y_true\", y[\"is_Sensing\"].values)\n",
    "    print(\"preds\", SorN_pred)\n",
    "\n",
    "    TorF_pred = TorF_model.predict(X)\n",
    "    print(\n",
    "        \"\\nThinking vs Feeling Accuracy: \", accuracy_score(y[\"is_Thinking\"], TorF_pred)\n",
    "    )\n",
    "    print(\"y_true\", y[\"is_Thinking\"].values)\n",
    "    print(\"preds\", TorF_pred)\n",
    "\n",
    "    JorP_pred = JorP_model.predict(X)\n",
    "    print(\n",
    "        \"\\nJudging vs Perceiving Accuracy: \", accuracy_score(y[\"is_Judging\"], JorP_pred)\n",
    "    )\n",
    "    print(\"y_true\", y[\"is_Judging\"].values)\n",
    "    print(\"preds\", JorP_pred)\n",
    "\n",
    "    # combining the predictions from the 4 models\n",
    "    result = combine_classes(EorI_pred, SorN_pred, TorF_pred, JorP_pred)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T09:10:06.500452Z",
     "end_time": "2023-04-22T09:11:25.673456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Preprocessing Time: 77.81932592391968 seconds\n",
      "\n",
      "Extrovert vs Introvert Accuracy:  0.6\n",
      "y_true [0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1\n",
      " 1 0 0]\n",
      "preds [0 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0\n",
      " 1 1 1]\n",
      "\n",
      "Sensing vs Intuition Accuracy:  0.625\n",
      "y_true [0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0\n",
      " 0 0 1]\n",
      "preds [1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 1 1 1]\n",
      "\n",
      "Thinking vs Feeling Accuracy:  0.85\n",
      "y_true [1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0\n",
      " 0 0 1]\n",
      "preds [1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0\n",
      " 0 1 1]\n",
      "\n",
      "Judging vs Perceiving Accuracy:  0.575\n",
      "y_true [0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1\n",
      " 0 0 0]\n",
      "preds [1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0\n",
      " 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(os.path.join(\"..\", \"data\", \"df_holdout.csv\"))\n",
    "y_truth = pd.read_csv(os.path.join(\"..\", \"data\", \"df_holdout.csv\"))[\"type\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T09:11:25.683021Z",
     "end_time": "2023-04-22T09:11:25.704055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    type                                              posts result\n0   INTP  'Well, there are two of my gang, so I can tell...   ISTJ\n1   ENTJ  'How long have you been with your human? ðŸ˜º|||E...   ENFJ\n2   ISFJ  '> obviously satirical im autistic|||why the f...   ESFP\n3   INTJ  '> i'm just stating the facts i can't even wal...   INTJ\n4   ENFJ  'I donâ€™t think this is a troll post. I know so...   ENFJ\n5   INTJ  'No, the government does a poor job of this, l...   ENTJ\n6   ENTP  'I wouldn't be attracted by rudeness|||Mine st...   ENTP\n7   INTJ  'For being too hot, man is the cause of global...   ISTP\n8   ENTJ  'Having little to no feelings and having to fa...   ISTJ\n9   ISTP  'Exactly this. People that Iâ€™m not close with ...   ENTJ\n10  ESFP  'My boyfriend is around 8 inches and it just t...   ENFJ\n11  INTJ  'They have no empirical basis, you cannot â€œmea...   INTP\n12  INFP  'that's a pretty good way to put it at the end...   INFP\n13  ESTJ  'Febreze gets me hyped every time|||Adventure ...   ENFJ\n14  ISFP  'good to know. was just mad confused about whe...   ENFP\n15  INTP  'Not my friend using it as his Discord server ...   INTJ\n16  ISTP  'Peanut butter works great as an emulsifier fo...   INTP\n17  INTJ  'One letter in difference is not a thing. They...   INFJ\n18  INFJ  'Really? I always thought there were only 2 ti...   INTP\n19  INTP  'Aahh I love infjs, its fun to talk with them ...   ESTJ\n20  ESTP  'From what i've seen from the way you describe...   INTP\n21  INFJ  'The Flow of Fortune. The capacity to bring in...   ENFJ\n22  INFP  'Succ Corsair and Awk Nova for me! Used to be ...   INFP\n23  INFP  'Nur weil es dir egal, heiÃŸt es nicht dass es ...   INFJ\n24  ENFP  'I looked it up, they just published the game ...   ENFP\n25  ENTP  'I don't think there's a problem with exploiti...   ENTP\n26  ISTJ  'Easterns literally shit in holes in the groun...   ESTP\n27  ISFP  'i think the eyeliner is too much IMO. i'd go ...   INFJ\n28  ENTJ  'Ma refeream strict la categoriile de persoane...   ENTJ\n29  INTP  'I always hated jeans as a kid, I would refuse...   INTP\n30  INTP  'You want to let us know how much is in the pi...   INTP\n31  ISTJ  'these are the same shoes. the cctv is just ba...   ENTP\n32  ENTP  'based on my experience with an esfp friend, s...   ISTP\n33  ENFP  'conversely there is a small but noticeably vo...   ENTP\n34  ENTP  'no no no ISTJs are so cute tho|||small apartm...   INTJ\n35  INFJ  'That there is enough resources for everyone o...   ENFJ\n36  ENFJ  'ENFJ, music and spirituality|||i didn't know ...   INFP\n37  ENFP  'Something can be nothing, a thing without any...   ESFJ\n38  INFP  'WAIT CHRIS IS SHORT?? ðŸ˜­ðŸ˜­|||No this is literal...   ESTP\n39  ISTP  'Ah entiendo! Gracias por explicarme. Entonces...   ESTJ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>posts</th>\n      <th>result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>INTP</td>\n      <td>'Well, there are two of my gang, so I can tell...</td>\n      <td>ISTJ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENTJ</td>\n      <td>'How long have you been with your human? ðŸ˜º|||E...</td>\n      <td>ENFJ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISFJ</td>\n      <td>'&gt; obviously satirical im autistic|||why the f...</td>\n      <td>ESFP</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>INTJ</td>\n      <td>'&gt; i'm just stating the facts i can't even wal...</td>\n      <td>INTJ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENFJ</td>\n      <td>'I donâ€™t think this is a troll post. I know so...</td>\n      <td>ENFJ</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>INTJ</td>\n      <td>'No, the government does a poor job of this, l...</td>\n      <td>ENTJ</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ENTP</td>\n      <td>'I wouldn't be attracted by rudeness|||Mine st...</td>\n      <td>ENTP</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>INTJ</td>\n      <td>'For being too hot, man is the cause of global...</td>\n      <td>ISTP</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>ENTJ</td>\n      <td>'Having little to no feelings and having to fa...</td>\n      <td>ISTJ</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ISTP</td>\n      <td>'Exactly this. People that Iâ€™m not close with ...</td>\n      <td>ENTJ</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>ESFP</td>\n      <td>'My boyfriend is around 8 inches and it just t...</td>\n      <td>ENFJ</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>INTJ</td>\n      <td>'They have no empirical basis, you cannot â€œmea...</td>\n      <td>INTP</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>INFP</td>\n      <td>'that's a pretty good way to put it at the end...</td>\n      <td>INFP</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>ESTJ</td>\n      <td>'Febreze gets me hyped every time|||Adventure ...</td>\n      <td>ENFJ</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>ISFP</td>\n      <td>'good to know. was just mad confused about whe...</td>\n      <td>ENFP</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>INTP</td>\n      <td>'Not my friend using it as his Discord server ...</td>\n      <td>INTJ</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>ISTP</td>\n      <td>'Peanut butter works great as an emulsifier fo...</td>\n      <td>INTP</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>INTJ</td>\n      <td>'One letter in difference is not a thing. They...</td>\n      <td>INFJ</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>INFJ</td>\n      <td>'Really? I always thought there were only 2 ti...</td>\n      <td>INTP</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>INTP</td>\n      <td>'Aahh I love infjs, its fun to talk with them ...</td>\n      <td>ESTJ</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>ESTP</td>\n      <td>'From what i've seen from the way you describe...</td>\n      <td>INTP</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>INFJ</td>\n      <td>'The Flow of Fortune. The capacity to bring in...</td>\n      <td>ENFJ</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>INFP</td>\n      <td>'Succ Corsair and Awk Nova for me! Used to be ...</td>\n      <td>INFP</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>INFP</td>\n      <td>'Nur weil es dir egal, heiÃŸt es nicht dass es ...</td>\n      <td>INFJ</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>ENFP</td>\n      <td>'I looked it up, they just published the game ...</td>\n      <td>ENFP</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>ENTP</td>\n      <td>'I don't think there's a problem with exploiti...</td>\n      <td>ENTP</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>ISTJ</td>\n      <td>'Easterns literally shit in holes in the groun...</td>\n      <td>ESTP</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>ISFP</td>\n      <td>'i think the eyeliner is too much IMO. i'd go ...</td>\n      <td>INFJ</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>ENTJ</td>\n      <td>'Ma refeream strict la categoriile de persoane...</td>\n      <td>ENTJ</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>INTP</td>\n      <td>'I always hated jeans as a kid, I would refuse...</td>\n      <td>INTP</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>INTP</td>\n      <td>'You want to let us know how much is in the pi...</td>\n      <td>INTP</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>ISTJ</td>\n      <td>'these are the same shoes. the cctv is just ba...</td>\n      <td>ENTP</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>ENTP</td>\n      <td>'based on my experience with an esfp friend, s...</td>\n      <td>ISTP</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>ENFP</td>\n      <td>'conversely there is a small but noticeably vo...</td>\n      <td>ENTP</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>ENTP</td>\n      <td>'no no no ISTJs are so cute tho|||small apartm...</td>\n      <td>INTJ</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>INFJ</td>\n      <td>'That there is enough resources for everyone o...</td>\n      <td>ENFJ</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>ENFJ</td>\n      <td>'ENFJ, music and spirituality|||i didn't know ...</td>\n      <td>INFP</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>ENFP</td>\n      <td>'Something can be nothing, a thing without any...</td>\n      <td>ESFJ</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>INFP</td>\n      <td>'WAIT CHRIS IS SHORT?? ðŸ˜­ðŸ˜­|||No this is literal...</td>\n      <td>ESTP</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>ISTP</td>\n      <td>'Ah entiendo! Gracias por explicarme. Entonces...</td>\n      <td>ESTJ</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"result\"] = predictions\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting for a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T09:11:25.688419Z",
     "end_time": "2023-04-22T09:11:25.891566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That somehow managed to be record short yet answer almost all the questions we would've asked, haha! Hi Deb! Welcome to Hou Tian; nice to meet you! I'm Jhenne, one of the mods here-- which means I gotta give you the modly speech :] Make sure to check out the Mandatory Reading up top! Our constantly updated Library is also a great resource, though it isn't mandatory reading-- we like to tell members to 'read as you need', rather than marathon read it all at once. One of the most helpful threads is the Gameplay So Far thread, which breaks down what all has gone down on the boards. (Now, the summary for January isn't tossed up yet, but we'd be happy to break down what you missed if you'd like.) I see that you're interested in Mai! That means both the Trying for a Canon Character page, and the Canon Character Rules and Consequences post will be helpful to check out. If you're ever considering an original character, we have our player-made adoptables list, and our factions, comprised of the Jade Shark/Bending Opposition, Original People of the Flame, and The Bending Crime Syndicates. As far as characters go, in the past tense I play Srai, a Jade Shark [s]that is very very dusty. In the Korraverse I play a reporter named Chihiro, and an ex-taxi dancer/wannabe actress named Naoki, and a Republic City University student named Haruna. I think that's it! If you have any questions, don't hesitate to ask a mod, or drop it right here in this thread so we can get back to you! Again, welcome! #CONFETTI\n",
      "                                         clean_posts  compound_sentiment   \n",
      "0  hat somehow managed record short yet answer al...              0.9834  \\\n",
      "\n",
      "   ADJ_avg  ADP_avg  ADV_avg  CONJ_avg  DET_avg  NOUN_avg  NUM_avg  PRT_avg   \n",
      "0       27       10       19        29       30        66        1        6  \\\n",
      "\n",
      "   ...  qm  em  colons  emojis  word_count  unique_words  upper  link_count   \n",
      "0  ...   0   8       1       0         271           180      6           0  \\\n",
      "\n",
      "   ellipses  img_count  \n",
      "0         0          0  \n",
      "\n",
      "[1 rows x 22 columns]\n",
      "Preprocessing Time: 0.1718766689300537 seconds\n"
     ]
    }
   ],
   "source": [
    "mbti = [\n",
    "    \"INFP\",\n",
    "    \"INFJ\",\n",
    "    \"INTP\",\n",
    "    \"INTJ\",\n",
    "    \"ENTP\",\n",
    "    \"enfp\",\n",
    "    \"ISTP\",\n",
    "    \"ISFP\",\n",
    "    \"ENTJ\",\n",
    "    \"ISTJ\",\n",
    "    \"ENFJ\",\n",
    "    \"ISFJ\",\n",
    "    \"ESTP\",\n",
    "    \"ESFP\",\n",
    "    \"ESFJ\",\n",
    "    \"ESTJ\",\n",
    "]\n",
    "tags_dict = {\n",
    "    \"ADJ_avg\": [\"JJ\", \"JJR\", \"JJS\"],\n",
    "    \"ADP_avg\": [\"EX\", \"TO\"],\n",
    "    \"ADV_avg\": [\"RB\", \"RBR\", \"RBS\", \"WRB\"],\n",
    "    \"CONJ_avg\": [\"CC\", \"IN\"],\n",
    "    \"DET_avg\": [\"DT\", \"PDT\", \"WDT\"],\n",
    "    \"NOUN_avg\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
    "    \"NUM_avg\": [\"CD\"],\n",
    "    \"PRT_avg\": [\"RP\"],\n",
    "    \"PRON_avg\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
    "    \"VERB_avg\": [\"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
    "    \".\": [\"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\"],\n",
    "    \"X\": [\"FW\", \"LS\", \"UH\"],\n",
    "}\n",
    "features = [\n",
    "    \"clean_posts\",\n",
    "    \"compound_sentiment\",\n",
    "    \"ADJ_avg\",\n",
    "    \"ADP_avg\",\n",
    "    \"ADV_avg\",\n",
    "    \"CONJ_avg\",\n",
    "    \"DET_avg\",\n",
    "    \"NOUN_avg\",\n",
    "    \"NUM_avg\",\n",
    "    \"PRT_avg\",\n",
    "    \"PRON_avg\",\n",
    "    \"VERB_avg\",\n",
    "    \"qm\",\n",
    "    \"em\",\n",
    "    \"colons\",\n",
    "    \"emojis\",\n",
    "    \"word_count\",\n",
    "    \"unique_words\",\n",
    "    \"upper\",\n",
    "    \"link_count\",\n",
    "    \"ellipses\",\n",
    "    \"img_count\",\n",
    "]\n",
    "\n",
    "\n",
    "def unique_words(s):\n",
    "    unique = set(s.split(\" \"))\n",
    "    return len(unique)\n",
    "\n",
    "\n",
    "def emojis(post):\n",
    "    # does not include emojis made purely from symbols, only :word:\n",
    "    emoji_count = 0\n",
    "    words = post.split()\n",
    "    for e in words:\n",
    "        if \"http\" not in e:\n",
    "            if e.count(\":\") == 2:\n",
    "                emoji_count += 1\n",
    "    return emoji_count\n",
    "\n",
    "\n",
    "def colons(post):\n",
    "    # Includes colons used in emojis\n",
    "    colon_count = 0\n",
    "    words = post.split()\n",
    "    for e in words:\n",
    "        if \"http\" not in e:\n",
    "            colon_count += e.count(\":\")\n",
    "    return colon_count\n",
    "\n",
    "\n",
    "def lemmitize(s):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_s = \"\"\n",
    "    for word in s.split(\" \"):\n",
    "        lemmatizer.lemmatize(word)\n",
    "        if word not in stopwords.words(\"english\"):\n",
    "            new_s += word + \" \"\n",
    "    return new_s[:-1]\n",
    "\n",
    "\n",
    "def clean(s):\n",
    "    # remove urls\n",
    "    s = re.sub(re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+).*\"), \"\", s)\n",
    "    # remove emails\n",
    "    s = re.sub(re.compile(r\"\\S+@\\S+\"), \"\", s)\n",
    "    # remove punctuation\n",
    "    s = re.sub(re.compile(r\"[^a-z\\s]\"), \"\", s)\n",
    "    # Make everything lowercase\n",
    "    s = s.lower()\n",
    "    # remove all personality types\n",
    "    for type_word in mbti:\n",
    "        s = s.replace(type_word.lower(), \"\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def prep_counts(s):\n",
    "    clean_s = clean(s)\n",
    "    d = {\n",
    "        \"clean_posts\": lemmitize(clean_s),\n",
    "        \"link_count\": s.count(\"http\"),\n",
    "        \"youtube\": s.count(\"youtube\") + s.count(\"youtu.be\"),\n",
    "        \"img_count\": len(re.findall(r\"(\\.jpg)|(\\.jpeg)|(\\.gif)|(\\.png)\", s)),\n",
    "        \"upper\": len([x for x in s.split() if x.isupper()]),\n",
    "        \"char_count\": len(s),\n",
    "        \"word_count\": clean_s.count(\" \") + 1,\n",
    "        \"qm\": s.count(\"?\"),\n",
    "        \"em\": s.count(\"!\"),\n",
    "        \"colons\": colons(s),\n",
    "        \"emojis\": emojis(s),\n",
    "        \"unique_words\": unique_words(clean_s),\n",
    "        \"ellipses\": len(re.findall(r\"\\.\\.\\.\\ \", s)),\n",
    "    }\n",
    "    return clean_s, d\n",
    "\n",
    "\n",
    "def prep_sentiment(s):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    score = analyzer.polarity_scores(s)\n",
    "    d = {\n",
    "        \"compound_sentiment\": score[\"compound\"],\n",
    "        \"pos_sentiment\": score[\"pos\"],\n",
    "        \"neg_sentiment\": score[\"neg\"],\n",
    "        \"neu_sentiment\": score[\"neu\"],\n",
    "    }\n",
    "    return d\n",
    "\n",
    "\n",
    "def tag_pos(s):\n",
    "    tagged_words = nltk.pos_tag(word_tokenize(s))\n",
    "    d = dict.fromkeys(tags_dict, 0)\n",
    "    for tup in tagged_words:\n",
    "        tag = tup[1]\n",
    "        for key, val in tags_dict.items():\n",
    "            if tag in val:\n",
    "                tag = key\n",
    "        d[tag] += 1\n",
    "    return d\n",
    "\n",
    "\n",
    "def prep_data(s):\n",
    "    clean_s, d = prep_counts(s)\n",
    "    d.update(prep_sentiment(lemmitize(clean_s)))\n",
    "    d.update(tag_pos(clean_s))\n",
    "    return pd.DataFrame([d])[features]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time()\n",
    "    string = \"That somehow managed to be record short yet answer almost all the questions we would've asked, haha! Hi Deb! Welcome to Hou Tian; nice to meet you! I'm Jhenne, one of the mods here-- which means I gotta give you the modly speech :] Make sure to check out the Mandatory Reading up top! Our constantly updated Library is also a great resource, though it isn't mandatory reading-- we like to tell members to 'read as you need', rather than marathon read it all at once. One of the most helpful threads is the Gameplay So Far thread, which breaks down what all has gone down on the boards. (Now, the summary for January isn't tossed up yet, but we'd be happy to break down what you missed if you'd like.) I see that you're interested in Mai! That means both the Trying for a Canon Character page, and the Canon Character Rules and Consequences post will be helpful to check out. If you're ever considering an original character, we have our player-made adoptables list, and our factions, comprised of the Jade Shark/Bending Opposition, Original People of the Flame, and The Bending Crime Syndicates. As far as characters go, in the past tense I play Srai, a Jade Shark [s]that is very very dusty. In the Korraverse I play a reporter named Chihiro, and an ex-taxi dancer/wannabe actress named Naoki, and a Republic City University student named Haruna. I think that's it! If you have any questions, don't hesitate to ask a mod, or drop it right here in this thread so we can get back to you! Again, welcome! #CONFETTI\"\n",
    "    print(string)\n",
    "    print(prep_data(string))\n",
    "    print(f\"Preprocessing Time: {time.time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T09:11:25.891566Z",
     "end_time": "2023-04-22T09:11:27.222134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTP\n",
      "Preprocessing Time: 1.32 seconds\n"
     ]
    }
   ],
   "source": [
    "def predict(s):\n",
    "    return len(s.split(\" \"))\n",
    "\n",
    "\n",
    "def predict_e(s):\n",
    "\n",
    "    X = prep_data(s)\n",
    "\n",
    "    # loading the 4 models\n",
    "    EorI_model = load(os.path.join(\"..\", \"models\", \"clf_is_Extrovert.joblib\"))\n",
    "    SorN_model = load(os.path.join(\"..\", \"models\", \"clf_is_Sensing.joblib\"))\n",
    "    TorF_model = load(os.path.join(\"..\", \"models\", \"clf_is_Thinking.joblib\"))\n",
    "    JorP_model = load(os.path.join(\"..\", \"models\", \"clf_is_Judging.joblib\"))\n",
    "\n",
    "    # predicting\n",
    "    EorI_pred = EorI_model.predict(X)\n",
    "    #print(\"preds\", EorI_pred)\n",
    "\n",
    "    SorN_pred = SorN_model.predict(X)\n",
    "    #print(\"preds\", SorN_pred)\n",
    "\n",
    "    TorF_pred = TorF_model.predict(X)\n",
    "    #print(\"preds\", TorF_pred)\n",
    "\n",
    "    JorP_pred = JorP_model.predict(X)\n",
    "    #print(\"preds\", JorP_pred)\n",
    "\n",
    "    # combining the predictions from the 4 models\n",
    "    result = combine_classes(EorI_pred, SorN_pred, TorF_pred, JorP_pred)\n",
    "\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time()\n",
    "    # sample test string. Type ISTP.\n",
    "    string = \"I plugged the data into tableau to see how the different features or how various mathematical formulas relate to the Weight. Once I had a few that didnâ€™t have a wide distribution, I just started trying different models, even ones we hadnâ€™t gone over yet. There are a LOT of regression models. I do not like this try everything method, itâ€™s inefficient and illogical.\"\n",
    "    print(predict_e(string))\n",
    "    print(f\"Preprocessing Time: {(time.time() - t):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73%\n",
      "74%\n",
      "75%\n",
      "76%\n",
      "77%\n",
      "78%\n",
      "79%\n",
      "80%\n",
      "81%\n",
      "82%\n",
      "83%\n",
      "84%\n",
      "85%\n",
      "86%\n",
      "87%\n",
      "88%\n",
      "89%\n",
      "90%\n",
      "91%\n",
      "92%\n",
      "93%\n",
      "94%\n",
      "95%\n",
      "96%\n",
      "97%\n",
      "98%\n",
      "99%\n",
      "3.3%\n",
      "18.25%\n",
      "36.19%\n",
      "31.6%\n",
      "10.65%\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "def check(str_to_check, expected_result):\n",
    "    index = 0\n",
    "    for i in range(len(expected_result)):\n",
    "        if expected_result[i] == str_to_check[i]:\n",
    "            index += 1\n",
    "    return index\n",
    "\n",
    "def get_progress():\n",
    "    progress_file = open(\"../data/progress.txt\", \"r\", encoding=\"UTF8\")\n",
    "    percentage = [0, 0, 0, 0, 0]\n",
    "    count = 0\n",
    "    for line in progress_file:\n",
    "        count += 1\n",
    "        percentage[int(line.replace(\"\\n\", \"\"))] += 1\n",
    "    progress_file.close()\n",
    "    return count, percentage\n",
    "\n",
    "def main():\n",
    "    with open(\"../data/texts_test.txt\", \"r\", encoding=\"UTF8\") as test_file:\n",
    "        with open(\"../data/progress.txt\", \"a\", encoding=\"UTF8\") as progress_file:\n",
    "            (already_count, percentage) = get_progress()\n",
    "            count = 0\n",
    "            prev_percent = 0\n",
    "            for line in test_file:\n",
    "                if count < already_count:\n",
    "                    count += 1\n",
    "                    continue\n",
    "                try:\n",
    "                    parts = line.split(\"|\")\n",
    "                    expected = parts[1]\n",
    "                    result = predict_e(parts[2].replace(\"\\n\", \"\"))\n",
    "                    count += 1\n",
    "                    index = check(result, expected)\n",
    "                    progress_file.write(f\"{index}\\n\")\n",
    "                    percentage[index] += 1\n",
    "                    if prev_percent < int(count * 100 / 102496):\n",
    "                        prev_percent = int(count * 100 / 102496)\n",
    "                        print(f\"{prev_percent}%\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            return count, percentage\n",
    "\n",
    "(count_sum, percentages) = main()\n",
    "\n",
    "for part in percentages:\n",
    "    print(f\"{round(part/count_sum * 100, 2)}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T19:27:06.337001Z",
     "end_time": "2023-04-23T01:00:02.025088Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T01:55:23.242117Z",
     "end_time": "2023-04-22T01:55:23.292917Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
