{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (MBTI) Myers-Briggs Personality Type Prediction\n",
    "\n",
    "* Extroversion vs. Introversion\n",
    "    * I - 0\n",
    "    * E - 1 \n",
    "    \n",
    "* Sensing vs. Intuition \n",
    "    * N - 0 \n",
    "    * S - 1\n",
    "    \n",
    "* Thinking vs. Feeling\n",
    "    * F - 0\n",
    "    * T - 1\n",
    "    \n",
    "* Judging vs. Perceiving\n",
    "    * P - 0\n",
    "    * J - 1 \n",
    "    \n",
    "## COUNTS & VECTORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:20:25.541108Z",
     "end_time": "2023-04-18T19:20:26.418173Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing dependencies here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# feature engineering\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# performance check\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:20:26.418173Z",
     "end_time": "2023-04-18T19:20:36.819996Z"
    }
   },
   "outputs": [],
   "source": [
    "# reading the clean_data_2 file\n",
    "personality_data = pd.read_csv(os.path.join(\"..\", \"data\", \"clean_data_2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:20:36.828508Z",
     "end_time": "2023-04-18T19:20:36.865621Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   type  is_Extrovert  is_Sensing  is_Thinking  is_Judging   \n0  INTJ             0           0            1           1  \\\n1  INTP             0           0            1           0   \n2  INFJ             0           0            0           1   \n3  INTP             0           0            1           0   \n4  INTJ             0           0            1           1   \n\n                                               posts   \n0  'Don’t peg your rate low, peg it high. An appe...  \\\n1  '...you get Kid Rock.|||Now it is. Plural: 2sh...   \n2  'To me it seems that Stoicism has been reduced...   \n3  'unBELIEVABLY based...same with physics|||yeah...   \n4  'Tbh, Ne doms are very fun chaotic people that...   \n\n                                         clean_posts  compound_sentiment   \n0   peg rate low  peg high  appeal expertise want...             0.99990  \\\n1      get kid rock   plural   shy happened skim ...             0.99995   \n2   seems stoicism reduced colloquial usage  beco...             0.99920   \n3   unbelievably based   physic yeah got material...             0.99970   \n4   tbh   doms fun chaotic people talk hour   mig...             0.99990   \n\n   pos_sentiment  neg_sentiment  ...  ADV_avg CONJ_avg DET_avg  NOUN_avg   \n0       0.332011       0.098166  ...      2.0      3.0     2.0       6.0  \\\n1       0.312169       0.088457  ...      4.0      7.0     5.0      13.0   \n2       0.304233       0.155340  ...      1.0      2.0     1.0       4.0   \n3       0.392857       0.081985  ...      1.0      2.0     1.0       5.0   \n4       0.416667       0.100324  ...      3.0      4.0     2.0       5.0   \n\n   NUM_avg  PRT_avg  PRON_avg  VERB_avg  ._avg  X_avg  \n0      0.0      0.0       3.0       6.0    4.0    0.0  \n1      0.0      0.0       4.0      13.0    8.0    0.0  \n2      0.0      0.0       2.0       3.0    2.0    0.0  \n3      0.0      0.0       2.0       4.0    1.0    0.0  \n4      0.0      0.0       3.5       6.0    2.0    0.0  \n\n[5 rows x 113 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>is_Extrovert</th>\n      <th>is_Sensing</th>\n      <th>is_Thinking</th>\n      <th>is_Judging</th>\n      <th>posts</th>\n      <th>clean_posts</th>\n      <th>compound_sentiment</th>\n      <th>pos_sentiment</th>\n      <th>neg_sentiment</th>\n      <th>...</th>\n      <th>ADV_avg</th>\n      <th>CONJ_avg</th>\n      <th>DET_avg</th>\n      <th>NOUN_avg</th>\n      <th>NUM_avg</th>\n      <th>PRT_avg</th>\n      <th>PRON_avg</th>\n      <th>VERB_avg</th>\n      <th>._avg</th>\n      <th>X_avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>INTJ</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>'Don’t peg your rate low, peg it high. An appe...</td>\n      <td>peg rate low  peg high  appeal expertise want...</td>\n      <td>0.99990</td>\n      <td>0.332011</td>\n      <td>0.098166</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>INTP</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>'...you get Kid Rock.|||Now it is. Plural: 2sh...</td>\n      <td>get kid rock   plural   shy happened skim ...</td>\n      <td>0.99995</td>\n      <td>0.312169</td>\n      <td>0.088457</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>13.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>13.0</td>\n      <td>8.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>INFJ</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>'To me it seems that Stoicism has been reduced...</td>\n      <td>seems stoicism reduced colloquial usage  beco...</td>\n      <td>0.99920</td>\n      <td>0.304233</td>\n      <td>0.155340</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>INTP</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>'unBELIEVABLY based...same with physics|||yeah...</td>\n      <td>unbelievably based   physic yeah got material...</td>\n      <td>0.99970</td>\n      <td>0.392857</td>\n      <td>0.081985</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>INTJ</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>'Tbh, Ne doms are very fun chaotic people that...</td>\n      <td>tbh   doms fun chaotic people talk hour   mig...</td>\n      <td>0.99990</td>\n      <td>0.416667</td>\n      <td>0.100324</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.5</td>\n      <td>6.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 113 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lookign at the top 5 rows of the dataset\n",
    "personality_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:20:36.859950Z",
     "end_time": "2023-04-18T19:20:36.875646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(3933, 113)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the number of rows and columns\n",
    "personality_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COUNTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question/Exclamation/Colon/Emoji Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:20:36.865621Z",
     "end_time": "2023-04-18T19:20:36.875646Z"
    }
   },
   "outputs": [],
   "source": [
    "def unique_words(s):\n",
    "    unique = set(s.split(\" \"))\n",
    "    return len(unique) / 50\n",
    "\n",
    "\n",
    "def emojis(post):\n",
    "    # does not include emojis made purely from symbols, only :word:\n",
    "    emoji_count = 0\n",
    "    words = post.split()\n",
    "    for e in words:\n",
    "        if \"http\" not in e:\n",
    "            if e.count(\":\") == 2:\n",
    "                emoji_count += 1\n",
    "    return emoji_count / 50\n",
    "\n",
    "\n",
    "def colons(post):\n",
    "    # Includes colons used in emojis\n",
    "    colon_count = 0\n",
    "    words = post.split()\n",
    "    for e in words:\n",
    "        if \"http\" not in e:\n",
    "            colon_count += e.count(\":\")\n",
    "    return colon_count / 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:20:36.875646Z",
     "end_time": "2023-04-18T19:20:45.782171Z"
    }
   },
   "outputs": [],
   "source": [
    "personality_data[\"qm\"] = personality_data[\"posts\"].apply(lambda s: s.count(\"?\") / 50)\n",
    "personality_data[\"em\"] = personality_data[\"posts\"].apply(lambda s: s.count(\"!\") / 50)\n",
    "personality_data[\"colons\"] = personality_data[\"posts\"].apply(colons)\n",
    "personality_data[\"emojis\"] = personality_data[\"posts\"].apply(emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:20:45.785338Z",
     "end_time": "2023-04-18T19:20:48.072147Z"
    }
   },
   "outputs": [],
   "source": [
    "personality_data[\"word_count\"] = personality_data[\"posts\"].apply(\n",
    "    lambda s: (s.count(\" \") + 1) / 50\n",
    ")\n",
    "personality_data[\"unique_words\"] = personality_data[\"posts\"].apply(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Stats\n",
    "\n",
    "* CAUTION - This will take Long !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:20:48.073186Z",
     "end_time": "2023-04-18T19:20:49.415001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 1.3342020511627197\n"
     ]
    }
   ],
   "source": [
    "# stats\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "# personality_data[\"avg_word_ct\"] = personality_data[\"word_count\"].apply(lambda s: s / 50)\n",
    "\n",
    "personality_data[\"post_length_var\"] = personality_data[\"posts\"].apply(\n",
    "    lambda x: np.var([len(post.split()) for post in x.split(\"|||\")])\n",
    ")\n",
    "\n",
    "print(f\"Time Taken: {time.time() - t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upper Case Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:20:49.431099Z",
     "end_time": "2023-04-18T19:20:51.342936Z"
    }
   },
   "outputs": [],
   "source": [
    "personality_data[\"upper\"] = personality_data[\"posts\"].apply(\n",
    "    lambda x: len([x for x in x.split() if x.isupper()]) / 50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:20:51.347843Z",
     "end_time": "2023-04-18T19:20:51.416411Z"
    }
   },
   "outputs": [],
   "source": [
    "personality_data[\"link_count\"] = personality_data[\"posts\"].apply(\n",
    "    lambda s: s.count(\"http\") / 50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ellipses Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:20:51.416411Z",
     "end_time": "2023-04-18T19:20:51.579224Z"
    }
   },
   "outputs": [],
   "source": [
    "ellipses_count = [\n",
    "    len(re.findall(r\"\\.\\.\\.\\ \", posts)) / 50 for posts in personality_data[\"posts\"]\n",
    "]\n",
    "personality_data[\"ellipses\"] = ellipses_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:20:51.584730Z",
     "end_time": "2023-04-18T19:20:57.905838Z"
    }
   },
   "outputs": [],
   "source": [
    "personality_data[\"img_count\"] = [\n",
    "    len(re.findall(r\"(\\.jpg)|(\\.jpeg)|(\\.gif)|(\\.png)\", post)) / 50\n",
    "    for post in personality_data[\"posts\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:20:57.905838Z",
     "end_time": "2023-04-18T19:20:57.927935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   type  is_Extrovert  is_Sensing  is_Thinking  is_Judging   \n0  INTJ             0           0            1           1  \\\n1  INTP             0           0            1           0   \n\n                                               posts   \n0  'Don’t peg your rate low, peg it high. An appe...  \\\n1  '...you get Kid Rock.|||Now it is. Plural: 2sh...   \n\n                                         clean_posts  compound_sentiment   \n0   peg rate low  peg high  appeal expertise want...             0.99990  \\\n1      get kid rock   plural   shy happened skim ...             0.99995   \n\n   pos_sentiment  neg_sentiment  ...    em colons emojis  word_count   \n0       0.332011       0.098166  ...  0.08   0.16    0.0       56.22  \\\n1       0.312169       0.088457  ...  0.20   2.04    0.0      136.48   \n\n   unique_words  post_length_var  upper  link_count  ellipses  img_count  \n0         24.66      6718.126848   2.06         0.0      0.04        0.0  \n1         48.64     15882.597104   6.98         0.0      0.34        0.0  \n\n[2 rows x 124 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>is_Extrovert</th>\n      <th>is_Sensing</th>\n      <th>is_Thinking</th>\n      <th>is_Judging</th>\n      <th>posts</th>\n      <th>clean_posts</th>\n      <th>compound_sentiment</th>\n      <th>pos_sentiment</th>\n      <th>neg_sentiment</th>\n      <th>...</th>\n      <th>em</th>\n      <th>colons</th>\n      <th>emojis</th>\n      <th>word_count</th>\n      <th>unique_words</th>\n      <th>post_length_var</th>\n      <th>upper</th>\n      <th>link_count</th>\n      <th>ellipses</th>\n      <th>img_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>INTJ</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>'Don’t peg your rate low, peg it high. An appe...</td>\n      <td>peg rate low  peg high  appeal expertise want...</td>\n      <td>0.99990</td>\n      <td>0.332011</td>\n      <td>0.098166</td>\n      <td>...</td>\n      <td>0.08</td>\n      <td>0.16</td>\n      <td>0.0</td>\n      <td>56.22</td>\n      <td>24.66</td>\n      <td>6718.126848</td>\n      <td>2.06</td>\n      <td>0.0</td>\n      <td>0.04</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>INTP</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>'...you get Kid Rock.|||Now it is. Plural: 2sh...</td>\n      <td>get kid rock   plural   shy happened skim ...</td>\n      <td>0.99995</td>\n      <td>0.312169</td>\n      <td>0.088457</td>\n      <td>...</td>\n      <td>0.20</td>\n      <td>2.04</td>\n      <td>0.0</td>\n      <td>136.48</td>\n      <td>48.64</td>\n      <td>15882.597104</td>\n      <td>6.98</td>\n      <td>0.0</td>\n      <td>0.34</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 124 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personality_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:20:57.927935Z",
     "end_time": "2023-04-18T19:20:57.943856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "type                object\nis_Extrovert         int64\nis_Sensing           int64\nis_Thinking          int64\nis_Judging           int64\n                    ...   \npost_length_var    float64\nupper              float64\nlink_count         float64\nellipses           float64\nimg_count          float64\nLength: 124, dtype: object"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the data types to make sure they still look good\n",
    "personality_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:20:57.933848Z",
     "end_time": "2023-04-18T19:20:58.022305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "type               0\nis_Extrovert       0\nis_Sensing         0\nis_Thinking        0\nis_Judging         0\n                  ..\npost_length_var    0\nupper              0\nlink_count         0\nellipses           0\nimg_count          0\nLength: 124, dtype: int64"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for null values again\n",
    "personality_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:20:57.951284Z",
     "end_time": "2023-04-18T19:21:13.709147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the data with counts\n",
    "personality_data.to_csv(os.path.join(\"..\", \"data\", \"clean_data_3.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize - For analysis purpose only. For model, the vectorization will be added to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-04-18T19:21:13.812487Z",
     "end_time": "2023-04-18T19:21:20.160357Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=25, max_df=0.8)\n",
    "tfidf_words = tfidf_vectorizer.fit_transform(personality_data[\"clean_posts\"])\n",
    "tfidf_vectorized_data = pd.DataFrame(\n",
    "    data=tfidf_words.toarray(), columns=tfidf_vectorizer.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:21:20.160357Z",
     "end_time": "2023-04-18T19:21:20.187917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   aaa  aaaa  aaron  aback  abandon  abandoned  abandoning  abandonment   \n0  0.0   0.0    0.0    0.0      0.0        0.0         0.0          0.0  \\\n1  0.0   0.0    0.0    0.0      0.0        0.0         0.0          0.0   \n2  0.0   0.0    0.0    0.0      0.0        0.0         0.0          0.0   \n3  0.0   0.0    0.0    0.0      0.0        0.0         0.0          0.0   \n4  0.0   0.0    0.0    0.0      0.0        0.0         0.0          0.0   \n\n   abbreviation  abc  ...  zodiac  zombie      zone     zoned  zoning  zoo   \n0        0.0000  0.0  ...     0.0     0.0  0.000000  0.000000     0.0  0.0  \\\n1        0.0181  0.0  ...     0.0     0.0  0.000000  0.000000     0.0  0.0   \n2        0.0000  0.0  ...     0.0     0.0  0.000000  0.000000     0.0  0.0   \n3        0.0000  0.0  ...     0.0     0.0  0.000000  0.000000     0.0  0.0   \n4        0.0000  0.0  ...     0.0     0.0  0.027067  0.050386     0.0  0.0   \n\n   zoom  zoomed  zuko  zur  \n0   0.0     0.0   0.0  0.0  \n1   0.0     0.0   0.0  0.0  \n2   0.0     0.0   0.0  0.0  \n3   0.0     0.0   0.0  0.0  \n4   0.0     0.0   0.0  0.0  \n\n[5 rows x 14016 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aaa</th>\n      <th>aaaa</th>\n      <th>aaron</th>\n      <th>aback</th>\n      <th>abandon</th>\n      <th>abandoned</th>\n      <th>abandoning</th>\n      <th>abandonment</th>\n      <th>abbreviation</th>\n      <th>abc</th>\n      <th>...</th>\n      <th>zodiac</th>\n      <th>zombie</th>\n      <th>zone</th>\n      <th>zoned</th>\n      <th>zoning</th>\n      <th>zoo</th>\n      <th>zoom</th>\n      <th>zoomed</th>\n      <th>zuko</th>\n      <th>zur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0181</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.027067</td>\n      <td>0.050386</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 14016 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorized_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:21:20.187917Z",
     "end_time": "2023-04-18T19:21:58.058708Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the TF-IDF vectorized data\n",
    "tfidf_vectorized_data.to_csv(os.path.join(\"..\", \"data\", \"tfidf_vectorized_data.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:21:58.063535Z",
     "end_time": "2023-04-18T19:22:05.471844Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(decode_error=\"ignore\", min_df=25, max_df=0.8,)\n",
    "\n",
    "count_words = count_vectorizer.fit_transform(personality_data[\"clean_posts\"])\n",
    "count_vectorized_data = pd.DataFrame(\n",
    "    data=count_words.toarray(), columns=count_vectorizer.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:22:05.476867Z",
     "end_time": "2023-04-18T19:22:05.511330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   aaa  aaaa  aaron  aback  abandon  abandoned  abandoning  abandonment   \n0    0     0      0      0        0          0           0            0  \\\n1    0     0      0      0        0          0           0            0   \n2    0     0      0      0        0          0           0            0   \n3    0     0      0      0        0          0           0            0   \n4    0     0      0      0        0          0           0            0   \n\n   abbreviation  abc  ...  zodiac  zombie  zone  zoned  zoning  zoo  zoom   \n0             0    0  ...       0       0     0      0       0    0     0  \\\n1             1    0  ...       0       0     0      0       0    0     0   \n2             0    0  ...       0       0     0      0       0    0     0   \n3             0    0  ...       0       0     0      0       0    0     0   \n4             0    0  ...       0       0     1      1       0    0     0   \n\n   zoomed  zuko  zur  \n0       0     0    0  \n1       0     0    0  \n2       0     0    0  \n3       0     0    0  \n4       0     0    0  \n\n[5 rows x 14016 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aaa</th>\n      <th>aaaa</th>\n      <th>aaron</th>\n      <th>aback</th>\n      <th>abandon</th>\n      <th>abandoned</th>\n      <th>abandoning</th>\n      <th>abandonment</th>\n      <th>abbreviation</th>\n      <th>abc</th>\n      <th>...</th>\n      <th>zodiac</th>\n      <th>zombie</th>\n      <th>zone</th>\n      <th>zoned</th>\n      <th>zoning</th>\n      <th>zoo</th>\n      <th>zoom</th>\n      <th>zoomed</th>\n      <th>zuko</th>\n      <th>zur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 14016 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorized_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T19:22:05.506321Z",
     "end_time": "2023-04-18T19:22:24.068510Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the Count vectorized data\n",
    "count_vectorized_data.to_csv(os.path.join(\"..\", \"data\", \"count_vectorized_data.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T19:22:24.070443Z",
     "end_time": "2023-04-18T19:22:24.074539Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
